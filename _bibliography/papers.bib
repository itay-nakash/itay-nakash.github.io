@inproceedings{nakash-etal-2025-breaking,
    title = "Breaking {R}e{A}ct Agents: Foot-in-the-Door Attack Will Get You In",
    author = "Nakash, Itay  and
      Kour, George  and
      Uziel, Guy  and
      Anaby Tavor, Ateret",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.363/",
    pages = "6484--6509",
    ISBN = "979-8-89176-195-7",
    selected={true},
    abstract = "Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become prevalent.As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack.Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions.Our results show that once a ReAct agent`s thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a {\textquoteleft}foot-in-the-door', allowing an attacker to embed malicious instructions into the agent`s thought process, making it more susceptible to harmful directives.To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks."
},

@misc{nakash2025adaptivocabenhancingllmefficiency,
      title={AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation}, 
      author={Itay Nakash and Nitay Calderon and Eyal Ben David and Elad Hoffer and Roi Reichart},
      year={2025},
      bibtex_show={true},
        abstract={Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance},
      eprint={2503.19693},
      selected={true},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.19693}, 
}


@misc{Kour2025pobs,
      title={Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models}, 
      author={George Kour and Itay Nakash and Ateret Anaby Tavor and Michal Shmueli-Scheuer},
      year={2025},
      bibtex_show={true},
        abstract={As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend.},
      eprint={2505.19621},
      selected={true},
      archivePrefix={arXiv},
      publisher = "Association for Computational Linguistics",
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.19621}, 
}

@article{nakash2025effective,
  title={Effective Red-Teaming of Policy-Adherent Agents},
  author={Nakash, Itay and Kour, George and Lazar, Koren and Vetzler, Matan and Uziel, Guy and Anaby-Tavor, Ateret},
  journal={arXiv preprint arXiv:2506.09600},
  year={2025},
  selected={true},
  primaryClass={cs.CL},
  year={2025}
}